names(rootNode) # return as many time as ever inter nodes are
rootNode[[1]] # whole first instance of first internal node
rootNode[[1]][[1]]
# xmlSApply()    :      it returns the actual value of that node i.e. inside the XML tag
#xmlSApply(rootNode2, xmlValue)
xmlSApply(rootNode, xmlValue)
## Using XPath
xmlSApply(rootNode, "//row", xmlValue)
rootNode
## Using XPath
#xmlSApply(rootNode, "/crossstreet", xmlValue)
xmlSApply(rootNode, "//name", xmlValue)
## Using XPath
#xpathSApply(rootNode, "/crossstreet", xmlValue)
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//price", xmlValue)
# XML of any random page source code
URL = "http://espn.go.com/nfl/team/_/name/baltimore-ravens"
doc <- htmlTreeParse(URL, useInternal=T)
doc <- htmlTreeParse(URL, useInternalNodes = T)
# XML of any random page source code
URL = "http://espn.go.com/nfl/team/_/name/baltimore-ravens"
doc <- htmlTreeParse(URL, useInternal = TRUE)
score <- xpathApply(doc, "//li@class='score'", xmlValue)
teams <- xpathApply(doc, "//li@class='team-name'", xmlValue)
library(XML)
URL = "https://www.mohfw.gov.in/"
cameraDataXML <- htmlTreeParse(URL, useInternal=TRUE)
# XML of any random page source code
URL = "http://espn.go.com/nfl/team/_/name/baltimore-ravens"
doc <- htmlTreeParse(URL, useInternal = TRUE)
URL = "https://www.mohfw.gov.in/"
doc <- htmlTreeParse(URL, useInternal = TRUE)
doc <- htmlTreeParse(URL, useInternal = TRUE, methods(curl))
library(xml2)
?htmlParse
?html_structure
#doc <- htmlTreeParse(URL, useInternal = TRUE, methods(curl))
doc <- read_html(URL)
html_structure(doc)
doc
d = xmlTreeParse(doc)
xmlRoot(d)
xmlRoot(doc)
htmlRoot(doc)
getwd()
class(system("ls"))
?system
dir()
getwd()
## Library
library(RMySQL)
install.packages(RMySQL)
install.packages('RMySQL')
## Library
library(RMySQL)
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
# executing query and disconnecting
result <- dbGetQuery(ucscDb, "get databases;"); dbDisconnect(ucscDb);
# executing query and disconnecting
result <- dbGetQuery(ucscDb, "get databases;")
## Library
library(RMySQL)
# connecting to server
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
# executing query and disconnecting
result <- dbGetQuery(ucscDb, "get databases;")
# executing query and disconnecting
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
result
getwd()
source('~/Documents/Data-Science-in-R/reader.R', echo=TRUE)
## Library
library(RMySQL)
# connecting to server
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
# executing query and disconnecting
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
result # list of the databases
# connecting to specifically 1 database     ---->     hg19
ucscDb <- dbConnect(MySQL(), user="genome", db="hg19", host="genome-mysql.cse.ucsc.edu")
allTables = dbListTables(hg19)
# connecting to specifically 1 database     ---->     hg19
hg19 <- dbConnect(MySQL(), user="genome", db="hg19", host="genome-mysql.cse.ucsc.edu")
allTables = dbListTables(hg19)
length(allTables)
class(allTables)
head(allTables)
allTables[11:15]
# fibding the Fields in a table of the current database
dbListFields(hg19, "affyU113Plus2")
# fibding the Fields in a table of the current database
dbListFields(hg19, "affyU133Plus2")
# fibding the Fields in a table of the current database
dbListFields(hg19, "affyU133Plus2")
dbListFields(hg19, "affyU133Plus2")
# fibding the Fields in a table of the current database
dbListFields(hg19, "affyCytoScan")
dbGetQuery(hg19, "select count(*) from affyCytoScan")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
# reading a TABLE
affyData <- dbReadTable(hg19, "affyU133Plus2")
class(affyData)
head(affyData)
# subset data
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch);
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMisSmall <- fetch()
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMisSmall <- fetch()
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch)
affyMisSmall <- fetch(query, n=10); dbClearResult(query)
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch) # still reading and will return other o/p when run again
# subset data
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query, n=10); quantile(affyMis$misMatch) # still reading and will return other o/p when run again
dim(affyMisSmall)
dbDisconnect(hg19)
getwd()
# r hdf5 pkg is installed from bioconductor
source("http://bioconductor.org/biocLite.R")
# r hdf5 pkg is installed from bioconductor
source("https://bioconductor.org/biocLite.R")
############################
# Installing Bioconductor
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install()
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week2/webscraping.R', echo=TRUE)
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week2/webscraping.R', echo=TRUE)
library(XML)
url = "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html = htmlTreeParse(url, useInternalNodes = T)
xpathApply(html, "//title", xmlValue)
xpathSApply(html, "//title", xmlValue)
url = "https://google.com/"
html = htmlTreeParse(url, useInternalNodes = T)
xpathSApply(html, "//title", xmlValue)
# looking any particular element
xpathSApply(html, "//td[@id='cidet by']", xmlValue)
library(httr)
library(httr)
url = "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html2 = GET(url)
content2 = content(html2, as='text')
parsedHTML = htmlParse(content2, asText = T)
xpathSApply(parsedHTML, "//title", xmlValue)
pg = GET("https://httpbin.org/basic-auth/user/passwd",
authenticate("user", "password")
)
pg
name(pg)
# without authentication
pg = GET("https://httpbin.org/basic-auth/user/passwd")
pg
# with authentication
pg = GET("https://httpbin.org/basic-auth/user/passwd",
authenticate("user", "passwd")
)
pg
name(pg)
names(pg)
google = handle("https://google.com")
google = handle("https://google.com")
pg1 = GET(handle = google, path="/")
pg2 = GET(handle = google, path="search")
getwd()
getwd()
system("ls ../../")
APIdata = read.csv("../../secrets.csv")
View(APIdata)
class(APIdata$API.key)
APIdata$API.key[1]
yourConsumerKey = as.character(APIdata$API.key[1])
yourConsumerSecret
yourConsumerKey
## using Application Programming Interfaces of twitter - Twitter API
## Getting the secret keys and tockens
APIdata = read.csv("../../secrets.csv")
View(APIdata)
yourConsumerKey = as.character(APIdata$API.key[1])
yourConsumerSecret = as.character(APIdata$API.secret.key[1])
yourToken = as.character(APIdata$Access.token[1])
yourTokenSecret = as.character(APIdata$Access.token.secret[1])
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week2/APIs.R', echo=TRUE)
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week2/APIs.R', echo=TRUE)
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week2/APIs.R', echo=TRUE)
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week2/APIs.R', echo=TRUE)
library(jsonlite)
json1 = content(homeTL)
json2 = fromJSON(toJSON(json1))
json2[1, 1:4]
json2
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week2/APIs.R', echo=TRUE)
cameraDataCSV <- read.table("../week1/downloaded/cameras.csv", sep = ",", header = TRUE)
# setting the working dir
newDir = "~/Documents/Data-Science-in-R/Getting and Cleaning Data/week4"
setwd(newDir)
cameraDataCSV <- read.table("../week1/downloaded/cameras.csv", sep = ",", header = TRUE)
colnames
cameraDataCSV <- read.table("../week1/downloaded/cameras.csv", sep = ",", header = TRUE)
colnames(camaraDataCSV)
cameraDataCSV <- read.table("../week1/downloaded/cameras.csv", sep = ",", header = TRUE)
colnames(cameraDataCSV)
names(cameraDataCSV)
### Converting to lowercase
tolower(names(cameraDataCSV))
library(stringr)
# spliting the strings
splitNum = strsplit(names(cameraDataCSV), "\\.")
splitNum[[5]]
splitNum[[6]]
splitNum[[6]][1]
#### selecting only first element of all the columns ---->  obvisly now  splitted into list
firstElement <- function(x){x[1]}
sapply(splitNames, firstElement)
### Converting to lowercase
tolower(names(cameraDataCSV))   # simalarlly toupper()
# spliting the strings
splitNum = strsplit(names(cameraDataCSV), "\\.")
splitNum[[5]]
splitNum[[6]]   # location is splitted now   --->    "Location" "1"
splitNum[[6]][1]  # first indexed val
#### selecting only first element of all the columns ---->  obvisly now  splitted into list
firstElement <- function(x){x[1]}
sapply(splitNames, firstElement)
sapply(splitNum, firstElement)
names(cameraDataCSV)
names(cameraDataCSV)        ## substituting '.'
sub("_", " ", "I_am_Ravi.")        ## substituting '_' with ' '
gsub("_", " ", "I_am_Ravi.")       ## replaced all '_'
url1 = "https://dl.dropboxusercontent.com/u7710864/data/reviews-apr29.csv"
url2 = "https://dl.dropboxusercontent.com/u7710864/data/solutions-apr29.csv"
url1 = "https://dl.dropboxusercontent.com/u7710864/data/reviews-apr29.csv"
url2 = "https://dl.dropboxusercontent.com/u7710864/data/solutions-apr29.csv"
download.file(url1, destfile = "./reviews.csv")
download.file(url2, destfile = "./reviews.csv")
reviews = read.csv("reviews.csv")
solutions = read.csv("solutions.csv")
download.file(url2, destfile = "./solutions.csv")
CPVID19.India.b4Lockdown = "https://raw.githubusercontent.com/ravi-prakash1907/COVID-19-India/master/before-lockdown/ready_to_use/India_dataset_dateWise_summary.csv"
download.file(CPVID19.India.b4Lockdown, destfile = "./indiaData.csv")
indiaData = read.csv("indiaData.csv")
grep("Delhi", indiaData$State)
CPVID19.India = "https://raw.githubusercontent.com/ravi-prakash1907/COVID-19-India/master/india-today/cleaned/statewise_ready.csv"
download.file(CPVID19.India, destfile = "./indiaData.csv")
indiaData = read.csv("indiaData.csv")
grep("Delhi", indiaData$State)
grep("Mizoram", indiaData$State)  #
table(grep("Mizoram", indiaData$State))
table(grepl("Mizoram", indiaData$State))
data <- indiaData[!table(grepl("Mizoram", indiaData$State)),]
data
data <- indiaData[!grepl("Mizoram", indiaData$State),]
data
grep("Mizoram", indiaData$State, value = T)  #  Index of 'Mizoram'
length(grep("Mizoram", indiaData$State))
nchar("Ravi Prakash")
substr("Ravi Prakash", 1, 4) # extract 'Ravi'
paste("Ravi", "Prakash", " ")
paste("Ravi", "Prakash", "-")
paste("Ravi", "Prakash")
paste("Ravi", "Prakash", sep="-")    # by default sep is " "
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week4/regex.R', echo=TRUE)
## Literals
text = "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
print(text)
Q1Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
Q1 <- read.csv(Q1Url)
Q1Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
Q1 <- read.csv(Q1Url)
View(Q1)
Q1_colnames <- names(Q1)
strsplit(Q1_colnames, "^wgtp")[[123]]
Q2_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q2_Path <- "/home/cabunic/Data Science/Coursera/3 - Getting and Cleaning Data/Week 4/Q3GDP.csv"
download.file(Q2_Url, Q2_Path, method = "curl")
Q2_Path <- "assgData/Q3GDP.csv"
Q2_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q2_Path <- "assgData/Q3GDP.csv"
download.file(Q2_Url, Q2_Path, method = "curl")
Q2_File <- read.csv(Q2_Path, nrow = 190, skip = 4)
Q2_File <- Q2_File[,c(1, 2, 4, 5)]
colnames(Q2_File) <- c("CountryCode", "Rank", "Country", "Total")
View(Q2_File)
Q2_File$Total <- as.integer(gsub(",", "", Q2_File$Total))
mean(Q2_File$Total, na.rm = T)
Q2_File$Country <- as.character(Q2_File$Country)
Q2_File$Country[99] <- "Côte d’Ivoire"
Q2_File$Country[186] <- "São Tomé and Príncipe"
Q2_File$Country[grep("^United", Q2_File$Country)]
library(data.table)
Q4GDP_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q4GDP_Path <- "assgData/Week3/Q3GDP.csv"
Q4Edu_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
Q4Edu_Path <- "assgData/Week3/Q3Edu.csv"
download.file(Q4GDP_Url, Q4GDP_Path, method = "curl")
Q4GDP_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q4GDP_Path <- "../week3/assgData/Q3GDP.csv"
Q4Edu_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
Q4Edu_Path <- "../week3/assgData/Q3Edu.csv"
download.file(Q4GDP_Url, Q4GDP_Path, method = "curl")
download.file(Q4Edu_Url, Q4Edu_Path, method = "curl")
Q4GDP <- fread(Q4GDP_Path, skip = 5, nrows = 190, select = c(1, 2, 4, 5), col.names = c("CountryCode", "Rank", "Economy", "Total"))
Q4Edu <- fread(Q4Edu_Path)
Q4_Merge <- merge(Q4GDP, Q4Edu, by = 'CountryCode')
View(Q4_Merge)
FiscalJune <- grep("Fiscal year end: June", Q4_Merge$`Special Notes`)
NROW(FiscalJune)
library(quantmod)
amzn = getSymbols("AMZN", auto.assign=FALSE)
sampleTimes = index(amzn)
library(quantmod)
library(lubridate)
amzn = getSymbols("AMZN", auto.assign=FALSE)
sampleTimes = index(amzn)
amzn2012 <- sampleTimes[grep("^2012", sampleTimes)]
NROW(amzn2012)
nrow(amzn2012[weekdays(amzn2012) == "Monday"])
nrow(amzn2012[wday(amzn2012) == "Monday"])
library(lubridate)
amzn = getSymbols("AMZN", auto.assign=FALSE)
sampleTimes = index(amzn)
amzn2012 <- sampleTimes[grep("^2012", sampleTimes)]
nrow(amzn2012)
library(quantmod)
amzn = getSymbols("AMZN", auto.assign=FALSE)
sampleTimes = index(amzn)
# How many values were collected in 2012? How many values were collected on Mondays in 2012?
library(quantmod)
library(lubridate)
amzn = getSymbols("AMZN", auto.assign=FALSE)
sampleTimes = index(amzn)
amzn2012 <- sampleTimes[grep("^2012", sampleTimes)]
nrow(amzn2012)
nrow(amzn2012[weekdays(amzn2012) == "Monday"])
library(quantmod)
library(lubridate)
amzn = getSymbols("AMZN", auto.assign=FALSE)
sampleTimes = index(amzn)
amzn2012 <- sampleTimes[grep("^2012", sampleTimes)]
nrow(amzn2012)
nrow(amzn2012[weekdays(amzn2012) == "Monday"])
Q1Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
Q1 <- read.csv(Q1Url)
View(Q1)
agricultureLogical <- Q1$ACR == 3 & Q1$AGS == 6
which(agricultureLogical)   # SOLUTION
library(jpeg)
Q2Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
Q2Path = 'assgData/Q2.jpg'
download.file(Q2Url, Q2Path, mode = 'wb')
Q2 <- readJPEG(Q2Path, native = TRUE)
quantile(Q2, probs = c(0.3, 0.8))
paste(quantile(Q2, probs = 0.3) - 638, quantile(Q2, probs = 0.8))  # SOLUTION
library(dplyr)
library(data.table)
Q3GDP_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q3GDP_Path <- "assgData/Q3GDP.csv"
Q3Edu_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
Q3Edu_Path <- "assgData/Q3Edu.csv"
download.file(Q3GDP_Url, Q3GDP_Path, method = "curl")
download.file(Q3Edu_Url, Q3Edu_Path, method = "curl")
Q3GDP <- fread(Q3GDP_Path, skip = 5, nrows = 190, select = c(1, 2, 4, 5), col.names = c("CountryCode", "Rank", "Economy", "Total"))
Q3Edu <- fread(Q3Edu_Path)
Q3GDP
View(Q3Edu)
Q3_Merge <- merge(Q3GDP, Q3Edu, by = 'CountryCode')
Q3_Merge <- Q3_Merge %>% arrange(desc(Rank))
View(Q3_Merge)
paste(nrow(Q3_Merge), " matches, 13th country is ", Q3_Merge$Economy[13])
Q3_Merge %>% group_by(`Income Group`) %>%
filter("High income: OECD" %in% `Income Group` | "High income: nonOECD" %in% `Income Group`) %>%
summarize(Average = mean(Rank, na.rm = T)) %>%
arrange(desc(`Income Group`))  # SOLUTION
Q3_Merge$RankGroups <- cut(Q3_Merge$Rank, breaks = 5)
vs <- table(Q3_Merge$RankGroups, Q3_Merge$`Income Group`)
View(vs)
vs[1, "Lower middle income"]
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week4/assignment.R', echo=TRUE)
# Step 2 #
# Extracting measurements on the mean and standard deviation
TidyData <- Merged_Data %>% select(subject, code, contains("mean"), contains("std"))
# Step 3 #
# attributes' name transformed in order to name the activities present in the dataset
TidyData$code <- activities[TidyData$code, 2]
# Step 4 #
# Labeling the data set with descriptive variable names
names(TidyData)[2] = "activity"
names(TidyData)<-gsub("Acc", "Accelerometer", names(TidyData))
names(TidyData)<-gsub("Gyro", "Gyroscope", names(TidyData))
names(TidyData)<-gsub("BodyBody", "Body", names(TidyData))
names(TidyData)<-gsub("Mag", "Magnitude", names(TidyData))
names(TidyData)<-gsub("^t", "Time", names(TidyData))
names(TidyData)<-gsub("^f", "Frequency", names(TidyData))
names(TidyData)<-gsub("tBody", "TimeBody", names(TidyData))
names(TidyData)<-gsub("-mean()", "Mean", names(TidyData), ignore.case = TRUE)
names(TidyData)<-gsub("-std()", "STD", names(TidyData), ignore.case = TRUE)
names(TidyData)<-gsub("-freq()", "Frequency", names(TidyData), ignore.case = TRUE)
names(TidyData)<-gsub("angle", "Angle", names(TidyData))
names(TidyData)<-gsub("gravity", "Gravity", names(TidyData))
# Step 5 #
# FinalData <- TidyData %>%
subject %>%
group_by(activity) %>%
summarise_all(funs(mean))
write.table(FinalData, "FinalData.txt", row.name=FALSE)
# Step 5 #
# FinalData <- TidyData %>%
group_by(subject, activity) %>%
summarise_all(funs(mean))
source('~/Documents/Data-Science-in-R/Getting and Cleaning Data/week4/assignment.R', echo=TRUE)
# Step 5 #
#
FinalData <- TidyData %>%
group_by(subject, activity) %>%
summarise_all(funs(mean))
write.table(FinalData, "FinalData.txt", row.name=FALSE)
# checking the structure
str(FinalData)
# Viewing the results
View(FinalData)
library(shiny)
runExample("01_hello") # a histogram
intToUtf8("\u2603")
"\u2603"
intToUtf8("☃")
utf8ToInt("☃")
string = " thank you for your full support buddy 🤗💓🙏"
string
strsplit(string, " ")
x = strsplit(string, " ")
x[[1]]
x[[[1]]]
x[1]
x[[[1]]]
x[[1]]
x[[1]][1]
x[[1]][2]
x[[1]][8]
x[[1]][9]
x[[1]][10]
x[[1]][10] = 🤗💓🙏
x[[1]][10] = 🤗💓🙏""
x[[1]][10] = "😍"
srting
string
string = "thank you for your full support buddy 😍"
for (word in strsplit(string, " ")) {
print(word)
}
string = "thank you for your full support buddy 😍❤️"
for (word in strsplit(string, " ")) {
print(word)
}
wList = NULL
for (word in strsplit(string, " ")) {
wList = c(wlist, word)
}
for (word in strsplit(string, " ")) {
wList = c(wList, word)
}
wList
length(wList)
wList(length(wList))
wList[length(wList)]
string
utf8ToInt('💓')
utf8ToInt('❤️')
swirl::swirl()
swirl::bye()
x = 4
f <- function(a) {
x = 3
print(x^g(a))
}
g <- function(a) {
a^2
}
f(2)
g <- function(a) {
x^a
}
f(2)
f(1)
x = 4
f <- function(a) {
x = 3
print(x+g(a))
}
g <- function(a) {}
g <- function(a) {
a+x
}
f(2)
f <- function(a) {
+ x = 3
+ print(g(a))
+ }
f <- function(a) {
x = 3
print(g(a))
}
f(2)
